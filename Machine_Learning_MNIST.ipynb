{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0xm3jrPiuxz"
      },
      "source": [
        "# import glob\r\n",
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEjCsPnXjq8e",
        "outputId": "a777ce3d-bb1f-45e5-eb6f-c07f9becee58"
      },
      "source": [
        "%cd '/content/drive/My Drive/Machine_Learning_MNIST/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Machine_Learning_MNIST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weKar3AJkX_r"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "\r\n",
        "import os\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.utils.data as Data\r\n",
        "import torchvision\r\n",
        "import time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn import svm\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "hDOG7LX2kgap",
        "outputId": "e355576c-9672-4db6-a73e-e7e86d4964e9"
      },
      "source": [
        "# show localtime\r\n",
        "localtime = time.asctime( time.localtime(time.time()) )\r\n",
        "print(\"The Localtime is:\", localtime)\r\n",
        "\r\n",
        "# Hyper Parameters\r\n",
        "EPOCH = 10  # train the training data n times, to save time, we just train 1 epoch\r\n",
        "BATCH_SIZE = 50\r\n",
        "LR = 0.001 # learning rate\r\n",
        "DOWNLOAD_MNIST = False\r\n",
        "\r\n",
        "# MNIST digits dataset\r\n",
        "if not(os.path.exists('./mnist/')) or not os.listdir('./mnist/'):\r\n",
        "  # if mnist dir is not created, create a MNIST dir and download MNIST dataset\r\n",
        "  DOWNLOAD_MNIST = True\r\n",
        "\r\n",
        "train_data = torchvision.datasets.MNIST(\r\n",
        "    root='./mnist/',\r\n",
        "    train=True,                                     # this is training data\r\n",
        "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\r\n",
        "    download=DOWNLOAD_MNIST,\r\n",
        ")\r\n",
        "\r\n",
        "# plot one example\r\n",
        "number_of_images_you_want_to_plot = 10\r\n",
        "print(train_data.train_data.size())     # (60000, 28, 28)\r\n",
        "print(train_data.train_labels.size())   # (60000)\r\n",
        "\r\n",
        "for i in range(1, number_of_images_you_want_to_plot + 1):\r\n",
        "  plt.subplot(2, 5, i)\r\n",
        "  plt.imshow(train_data.train_data[i].numpy(), cmap='gray')\r\n",
        "  plt.title('%i' % train_data.train_labels[i])\r\n",
        "  plt.xticks([])\r\n",
        "  plt.yticks([])\r\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Localtime is: Mon Jan 25 07:02:32 2021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rIviqZvmboK",
        "outputId": "46b75cd4-9258-4ca4-9e1f-aa6191dc8a4e"
      },
      "source": [
        "# pick 2000 samples to speed up testing\r\n",
        "train_data = torchvision.datasets.MNIST(root='./mnist/', train=True)\r\n",
        "train_x = torch.unsqueeze(train_data.train_data, dim=1).type(torch.FloatTensor)[:60000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\r\n",
        "train_y = train_data.train_labels[:60000]\r\n",
        "\r\n",
        "# pick 2000 samples to speed up testing\r\n",
        "test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)\r\n",
        "test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\r\n",
        "test_y = test_data.test_labels[:2000]\r\n",
        "\r\n",
        "print(train_x.size(),train_y.size(),test_x.size(),test_y.size())\r\n",
        "\r\n",
        "train_x = train_x.view(-1,28*28)\r\n",
        "test_x = test_x.view(-1,28*28)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 1, 28, 28]) torch.Size([60000]) torch.Size([2000, 1, 28, 28]) torch.Size([2000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:63: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyW0wLkimjxB"
      },
      "source": [
        "# K-Nearest Neighbour Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "0Lv4XPx0mhHq",
        "outputId": "a1df6935-b47b-488e-d8b5-53a8fd37511c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn import datasets\r\n",
        "from skimage import exposure\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import imutils\r\n",
        "import cv2\r\n",
        "\r\n",
        "trainData = np.array(train_x)\r\n",
        "testData = np.array(test_x)\r\n",
        "trainLabels = np.array(train_y)\r\n",
        "testLabels = np.array(test_y)\r\n",
        "valData = testData\r\n",
        "valLabels = testLabels\r\n",
        "\r\n",
        "# Checking sizes of each data split\r\n",
        "print(\"training data points: {}\".format(len(trainLabels)))\r\n",
        "print(\"validation data points: {}\".format(len(valLabels)))\r\n",
        "print(\"testing data points: {}\".format(len(testLabels)))\r\n",
        "\r\n",
        "# initialize the values of k for our k-Nearest Neighbor classifier along with the\r\n",
        "# list of accuracies for each value of k\r\n",
        "kVals = range(1, 30, 2)\r\n",
        "accuracies = []\r\n",
        "'''\r\n",
        "*********** Currently Choosing The Best Value of K ***********\r\n",
        "When K = 1, accuracy is 96.00%\r\n",
        "The Localtime is:  Mon Jan 25 03:56:22 2021\r\n",
        "When K = 3, accuracy is 95.80%\r\n",
        "The Localtime is:  Mon Jan 25 03:59:52 2021\r\n",
        "When K = 5, accuracy is 95.65%\r\n",
        "The Localtime is:  Mon Jan 25 04:03:23 2021\r\n",
        "When K = 7, accuracy is 95.65%\r\n",
        "The Localtime is:  Mon Jan 25 04:06:53 2021\r\n",
        "When K = 9, accuracy is 95.00%\r\n",
        "The Localtime is:  Mon Jan 25 04:10:24 2021\r\n",
        "When K = 11, accuracy is 95.05%\r\n",
        "The Localtime is:  Mon Jan 25 04:13:54 2021\r\n",
        "When K = 13, accuracy is 94.85%\r\n",
        "The Localtime is:  Mon Jan 25 04:17:24 2021\r\n",
        "When K = 15, accuracy is 94.50%\r\n",
        "The Localtime is:  Mon Jan 25 04:20:53 2021\r\n",
        "When K = 17, accuracy is 94.80%\r\n",
        "The Localtime is:  Mon Jan 25 04:24:23 2021\r\n",
        "When K = 19, accuracy is 94.85%\r\n",
        "The Localtime is:  Mon Jan 25 04:27:54 2021\r\n",
        "When K = 21, accuracy is 94.70%\r\n",
        "The Localtime is:  Mon Jan 25 04:31:24 2021\r\n",
        "When K = 23, accuracy is 94.60%\r\n",
        "The Localtime is:  Mon Jan 25 04:34:54 2021\r\n",
        "When K = 25, accuracy is 94.35%\r\n",
        "The Localtime is:  Mon Jan 25 04:38:25 2021\r\n",
        "When K = 27, accuracy is 94.30%\r\n",
        "The Localtime is:  Mon Jan 25 04:41:57 2021\r\n",
        "When K = 29, accuracy is 94.05%\r\n",
        "The Localtime is:  Mon Jan 25 04:45:28 2021\r\n",
        "k=1 achieved highest accuracy of 96.00% on validation data\r\n",
        "'''\r\n",
        "# print('*********** Currently Choosing The Best Value of K ***********')\r\n",
        "\r\n",
        "# # loop over kVals\r\n",
        "# for k in range(1, 30, 2):\r\n",
        "#   # train the classifier with the current value of `k`\r\n",
        "#   model = KNeighborsClassifier(n_neighbors=k)\r\n",
        "#   model.fit(trainData, trainLabels)\r\n",
        "\r\n",
        "#   # evaluate the model and print the accuracies list\r\n",
        "#   score = model.score(valData, valLabels)\r\n",
        "#   print(\"When K = %d, accuracy is %.2f%%\" % (k, score * 100))\r\n",
        "#   accuracies.append(score)\r\n",
        "#   localtime = time.asctime( time.localtime(time.time()) )\r\n",
        "#   print(\"The Localtime is: \", localtime)\r\n",
        "\r\n",
        "# # largest accuracy\r\n",
        "# i = np.argmax(accuracies) # np.argmax returns the indices of the maximum values along an axis\r\n",
        "# print(\"k=%d achieved highest accuracy of %.2f%% on validation data\" % (kVals[i], accuracies[i] * 100))\r\n",
        "\r\n",
        "# Now that I know the best value of k, re-train the classifier\r\n",
        "model = KNeighborsClassifier(n_neighbors=1) # kVals[i]\r\n",
        "model.fit(trainData, trainLabels)\r\n",
        "\r\n",
        "# Predict labels for the test set\r\n",
        "predictions = model.predict(testData)\r\n",
        "\r\n",
        "# Evaluate performance of model for each of the digits\r\n",
        "print(\"EVALUATION ON TESTING DATA\")\r\n",
        "print(classification_report(testLabels, predictions))\r\n",
        "\r\n",
        "'''\r\n",
        "some indices are classified correctly 100% of the time (precision = 1)\r\n",
        "high accuracy (98%)\r\n",
        "check predictions against images\r\n",
        "loop over a few random digits\r\n",
        "'''\r\n",
        "# image = testData\r\n",
        "# j = 0\r\n",
        "# for i in np.random.randint(0, high=len(testLabels), size=(24,)):  \r\n",
        "#   prediction = model.predict(image)[i]\r\n",
        "#   image0 = image[i].reshape((28, 28)).astype(\"uint8\") # convert the image for a 64-dim array to an 8 x 8 image compatible with OpenCV\r\n",
        "#   image0 = exposure.rescale_intensity(image0, out_range=(0, 255))\r\n",
        "#   plt.subplot(4,6,j+1)\r\n",
        "#   plt.title(str(prediction))\r\n",
        "#   plt.imshow(image0,cmap='gray')\r\n",
        "#   plt.axis('off')\r\n",
        "#   j = j+1\r\n",
        "# plt.show() \r\n",
        "        \r\n",
        "# image0 = imutils.resize(image[0], width=32, inter=cv2.INTER_CUBIC) # then resize it to 32 x 32 pixels for better visualization\r\n",
        "# # show the prediction\r\n",
        "# print(\"I think that digit is: {}\".format(prediction))\r\n",
        "# print('image0 is ',image0)\r\n",
        "# cv2.imshow(\"Image\", image0)\r\n",
        "# cv2.waitKey(0) # press enter to view each one!\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data points: 60000\n",
            "validation data points: 2000\n",
            "testing data points: 2000\n",
            "EVALUATION ON TESTING DATA\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       175\n",
            "           1       0.97      0.99      0.98       234\n",
            "           2       0.99      0.96      0.97       219\n",
            "           3       0.94      0.96      0.95       207\n",
            "           4       0.96      0.95      0.96       217\n",
            "           5       0.93      0.96      0.95       179\n",
            "           6       0.98      0.98      0.98       178\n",
            "           7       0.94      0.94      0.94       205\n",
            "           8       0.98      0.92      0.95       192\n",
            "           9       0.94      0.94      0.94       194\n",
            "\n",
            "    accuracy                           0.96      2000\n",
            "   macro avg       0.96      0.96      0.96      2000\n",
            "weighted avg       0.96      0.96      0.96      2000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsome indices are classified correctly 100% of the time (precision = 1)\\nhigh accuracy (98%)\\ncheck predictions against images\\nloop over a few random digits\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RskYC2gaopHr"
      },
      "source": [
        "# Logistic Regression Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjLbPcfHon46",
        "outputId": "f0ed77ee-fc4e-456f-fc41-339a7b488eb3"
      },
      "source": [
        "EPOCH = 10               # train the training data n times, to save time, we just train 1 epoch\r\n",
        "BATCH_SIZE = 50\r\n",
        "LR = 0.001  \r\n",
        "DOWNLOAD_MNIST = False\r\n",
        "\r\n",
        "# Mnist digits dataset\r\n",
        "if not(os.path.exists('./mnist/')) or not os.listdir('./mnist/'):\r\n",
        "  DOWNLOAD_MNIST = True\r\n",
        "\r\n",
        "train_data = torchvision.datasets.MNIST(\r\n",
        "  root='./mnist/',\r\n",
        "  train=True,                                     # this is training data\r\n",
        "  transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\r\n",
        "  download=DOWNLOAD_MNIST,\r\n",
        ")\r\n",
        "\r\n",
        "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\r\n",
        "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\r\n",
        "\r\n",
        "# pick 2000 samples to speed up testing\r\n",
        "test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)\r\n",
        "test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\r\n",
        "test_y = test_data.test_labels[:2000]\r\n",
        "\r\n",
        "class logisticRg(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(logisticRg, self).__init__()\r\n",
        "    self.lr = nn.Sequential(\r\n",
        "        nn.Linear(28*28,10))\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    output = self.lr(x)\r\n",
        "    return output, x    # return x for visualization\r\n",
        "\r\n",
        "lor = logisticRg()\r\n",
        "print(lor)  # net architecture\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(lor.parameters(), lr=LR)   # optimize all logistic parameters\r\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\r\n",
        "\r\n",
        "# training and testing\r\n",
        "for epoch in range(EPOCH):\r\n",
        "  for step, (b_x, b_y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\r\n",
        "    b_x = b_x.view(-1, 28*28)    \r\n",
        "\r\n",
        "    output = lor(b_x)[0]            # logistic output\r\n",
        "    loss = loss_func(output, b_y)   # cross entropy loss\r\n",
        "    optimizer.zero_grad()           # clear gradients for this training step\r\n",
        "    loss.backward()                 # backpropagation, compute gradients\r\n",
        "    optimizer.step()                # apply gradients\r\n",
        "\r\n",
        "    if step % 50 == 0:\r\n",
        "      test_output, last_layer = lor(test_x.view(-1,28*28))\r\n",
        "      pred_y = torch.max(test_output, 1)[1].data.numpy()\r\n",
        "      accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))\r\n",
        "      print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\r\n",
        "     \r\n",
        "# print 10 predictions from test data\r\n",
        "test_output, _ = lor(test_x[:10].view(-1,28*28))\r\n",
        "pred_y = torch.max(test_output, 1)[1].data.numpy()\r\n",
        "print(pred_y, 'prediction number')\r\n",
        "print(test_y[:10].numpy(), 'real number')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:63: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logisticRg(\n",
            "  (lr): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch:  0 | train loss: 2.3424 | test accuracy: 0.11\n",
            "Epoch:  0 | train loss: 1.3019 | test accuracy: 0.75\n",
            "Epoch:  0 | train loss: 0.8522 | test accuracy: 0.78\n",
            "Epoch:  0 | train loss: 0.6557 | test accuracy: 0.82\n",
            "Epoch:  0 | train loss: 0.5167 | test accuracy: 0.83\n",
            "Epoch:  0 | train loss: 0.5692 | test accuracy: 0.84\n",
            "Epoch:  0 | train loss: 0.5703 | test accuracy: 0.85\n",
            "Epoch:  0 | train loss: 0.5245 | test accuracy: 0.85\n",
            "Epoch:  0 | train loss: 0.4314 | test accuracy: 0.85\n",
            "Epoch:  0 | train loss: 0.4675 | test accuracy: 0.86\n",
            "Epoch:  0 | train loss: 0.5557 | test accuracy: 0.86\n",
            "Epoch:  0 | train loss: 0.3749 | test accuracy: 0.87\n",
            "Epoch:  0 | train loss: 0.4534 | test accuracy: 0.87\n",
            "Epoch:  0 | train loss: 0.3840 | test accuracy: 0.87\n",
            "Epoch:  0 | train loss: 0.2894 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.4069 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.2029 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.2292 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.4612 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.2564 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.4091 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.2904 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.3880 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.1784 | test accuracy: 0.88\n",
            "Epoch:  1 | train loss: 0.3149 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2470 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.4533 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.3253 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2412 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.1979 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.3642 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.3070 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.1853 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.3512 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2889 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.1735 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.5248 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2830 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.1422 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.3395 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2872 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.3837 | test accuracy: 0.90\n",
            "Epoch:  1 | train loss: 0.3348 | test accuracy: 0.90\n",
            "Epoch:  1 | train loss: 0.2718 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2117 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2021 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2444 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.4131 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.1269 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.1464 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.2796 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.3596 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.2554 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.2578 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.2133 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.4085 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.6160 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.1786 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3900 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.4856 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.2135 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.2260 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3070 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.2439 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3023 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3560 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.2403 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3426 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3556 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3454 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3133 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.4064 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.3259 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.1125 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.5431 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.6188 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.1867 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.0979 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2016 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2772 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.4054 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.3209 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.4023 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.4349 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2367 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.4030 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.1668 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2212 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.4513 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.1620 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2870 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.6717 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.4102 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.3864 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.5411 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.1677 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.4497 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.3535 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.4109 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2229 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.3340 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2599 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.1769 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.3249 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2426 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.3933 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.7212 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.0940 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.0961 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2266 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.4973 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2604 | test accuracy: 0.91\n",
            "Epoch:  4 | train loss: 0.3010 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2711 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.5467 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2467 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2815 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.3276 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.1816 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.3142 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.1814 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2190 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.3508 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2565 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.3916 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.5020 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2474 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2041 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.4071 | test accuracy: 0.91\n",
            "Epoch:  5 | train loss: 0.4426 | test accuracy: 0.91\n",
            "Epoch:  5 | train loss: 0.1606 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2023 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2174 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.1863 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2584 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2416 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.1861 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.3896 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.3383 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.1678 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.0997 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.1761 | test accuracy: 0.91\n",
            "Epoch:  5 | train loss: 0.0786 | test accuracy: 0.91\n",
            "Epoch:  5 | train loss: 0.1973 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.1593 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.1301 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.3411 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.2165 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.2090 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.3479 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.6730 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.3793 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.6005 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.2088 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.3223 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.2519 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.3536 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.3254 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.2681 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.0848 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.6467 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.4038 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.0884 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.1470 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.1644 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.1951 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.5163 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.2683 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.5687 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.4677 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.1701 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.0822 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.2091 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.0887 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.1808 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.1136 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.2526 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.3332 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.1968 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.3931 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.4380 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.1634 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.2783 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.2730 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.3063 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.1442 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.1250 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.1918 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.3829 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.2634 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.2145 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.1556 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.1822 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1472 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.3343 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.3760 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.3934 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.3232 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.1960 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.2807 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1796 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.3305 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1199 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.0833 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.5191 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.2788 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.2709 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.1356 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1706 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.2899 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.2807 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1525 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.3018 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.3289 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1921 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.1696 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.1217 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.3645 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.1413 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.1520 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.1907 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.2978 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.2753 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.5107 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.2909 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.1555 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.0915 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.2898 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.1232 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.3360 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.1558 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.2403 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.1555 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.4180 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.3279 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.4353 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.1004 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.1968 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.2804 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.2525 | test accuracy: 0.90\n",
            "[7 2 1 0 4 1 4 9 6 9] prediction number\n",
            "[7 2 1 0 4 1 4 9 5 9] real number\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTmGX6VyqOrv"
      },
      "source": [
        "# Support Vector Machine (SVM) Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6p5C4BnqOOC",
        "outputId": "d5af76b7-ea31-48a8-bd4d-94fddb3f3346"
      },
      "source": [
        "import argparse\r\n",
        "import torchvision\r\n",
        "from sklearn import svm\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "# Load the training data\r\n",
        "def MNIST_DATASET_TRAIN(downloads, train_amount):\r\n",
        "  # Load dataset\r\n",
        "  training_data = torchvision.datasets.MNIST(\r\n",
        "      root = './mnist/',\r\n",
        "      train = True,\r\n",
        "      transform = torchvision.transforms.ToTensor(),\r\n",
        "      download = downloads\r\n",
        "      ) \r\n",
        "\r\n",
        "  #Convert Training data to numpy\r\n",
        "  train_data = training_data.train_data.numpy()[:train_amount]\r\n",
        "  train_label = training_data.train_labels.numpy()[:train_amount]\r\n",
        "\r\n",
        "  # Print training data size\r\n",
        "  print('Training data size: ',train_data.shape)\r\n",
        "  print('Training data label size:',train_label.shape)    \r\n",
        "  train_data = train_data/255.0\r\n",
        "  \r\n",
        "  return train_data, train_label\r\n",
        "\r\n",
        "# Load the test data\r\n",
        "def MNIST_DATASET_TEST(downloads, test_amount):\r\n",
        "  testing_data = torchvision.datasets.MNIST(\r\n",
        "      root = './mnist/',\r\n",
        "      train = False,\r\n",
        "      transform = torchvision.transforms.ToTensor(),\r\n",
        "      download = downloads\r\n",
        "      )\r\n",
        "    \r\n",
        "  # Convert Testing data to numpy\r\n",
        "  test_data = testing_data.test_data.numpy()[:test_amount]\r\n",
        "  test_label = testing_data.test_labels.numpy()[:test_amount]\r\n",
        "  \r\n",
        "  # Print training data size\r\n",
        "  print('test data size: ',test_data.shape)\r\n",
        "  print('test data label size:',test_label.shape)     \r\n",
        "  test_data = test_data/255.0\r\n",
        "  \r\n",
        "  return test_data, test_label\r\n",
        "\r\n",
        "# Load Training Data & Testing Data\r\n",
        "train_data, train_label = MNIST_DATASET_TRAIN(True, 60000)\r\n",
        "test_data, test_label = MNIST_DATASET_TEST(True, 2000)\r\n",
        "\r\n",
        "training_features = train_data.reshape(60000,-1)\r\n",
        "test_features = test_data.reshape(2000,-1)\r\n",
        "\r\n",
        "# Training SVM\r\n",
        "print('------Training and testing SVM------')\r\n",
        "clf = svm.SVC(C=5, gamma=0.05,max_iter=10)\r\n",
        "clf.fit(training_features, train_label)\r\n",
        "\r\n",
        "#Test on test data\r\n",
        "test_result = clf.predict(test_features)\r\n",
        "precision = sum(test_result == test_label)/test_label.shape[0]\r\n",
        "print('Test precision: ', precision)\r\n",
        "\r\n",
        "#Test on Training data\r\n",
        "train_result = clf.predict(training_features)\r\n",
        "precision = sum(train_result == train_label)/train_label.shape[0]\r\n",
        "print('Training precision: ', precision)\r\n",
        "\r\n",
        "#Show the confusion matrix\r\n",
        "matrix = confusion_matrix(test_label, test_result)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data size:  (60000, 28, 28)\n",
            "Training data label size: (60000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:63: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test data size:  (2000, 28, 28)\n",
            "test data label size: (2000,)\n",
            "------Training and testing SVM------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test precision:  0.5155\n",
            "Training precision:  0.52875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VoQM5tzr5-A"
      },
      "source": [
        "# Multi-Layer Perceptron Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtZatWPxr42q",
        "outputId": "36ea4d45-020f-40c2-9a2f-705edd01bdac"
      },
      "source": [
        "class MLP(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(MLP, self).__init__()\r\n",
        "    self.mlp = nn.Sequential(\r\n",
        "        nn.Linear(28*28,28*28),\r\n",
        "        nn.Linear(28*28,10))\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    output = self.mlp(x)\r\n",
        "    return output, x    # return x for visualization\r\n",
        "\r\n",
        "mlp = MLP()\r\n",
        "print(mlp)  # net architecture\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=LR)   # optimize all logistic parameters\r\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\r\n",
        "\r\n",
        "# training and testing\r\n",
        "for epoch in range(EPOCH):\r\n",
        "  for step, (b_x, b_y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\r\n",
        "    b_x = b_x.view(-1, 28*28)  \r\n",
        "\r\n",
        "    output = mlp(b_x)[0]            # logistic output\r\n",
        "    loss = loss_func(output, b_y)   # cross entropy loss\r\n",
        "    optimizer.zero_grad()           # clear gradients for this training step\r\n",
        "    loss.backward()                 # backpropagation, compute gradients\r\n",
        "    optimizer.step()                # apply gradients\r\n",
        "\r\n",
        "    if step % 50 == 0:\r\n",
        "      test_output, last_layer = mlp(test_x.view(-1,28*28))\r\n",
        "      pred_y = torch.max(test_output, 1)[1].data.numpy()\r\n",
        "      accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))\r\n",
        "      print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\r\n",
        "          \r\n",
        "# print 10 predictions from test data\r\n",
        "test_output, _ = mlp(test_x[:10].view(-1,28*28))\r\n",
        "pred_y = torch.max(test_output, 1)[1].data.numpy()\r\n",
        "print(pred_y, 'prediction number')\r\n",
        "print(test_y[:10].numpy(), 'real number')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=784, bias=True)\n",
            "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch:  0 | train loss: 2.3128 | test accuracy: 0.22\n",
            "Epoch:  0 | train loss: 0.4413 | test accuracy: 0.85\n",
            "Epoch:  0 | train loss: 0.3303 | test accuracy: 0.86\n",
            "Epoch:  0 | train loss: 0.2336 | test accuracy: 0.87\n",
            "Epoch:  0 | train loss: 0.4194 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.7033 | test accuracy: 0.86\n",
            "Epoch:  0 | train loss: 0.2985 | test accuracy: 0.87\n",
            "Epoch:  0 | train loss: 0.3241 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.2732 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.1004 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.5852 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.1024 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.3480 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.4061 | test accuracy: 0.87\n",
            "Epoch:  0 | train loss: 0.2526 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.1243 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.6399 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.3640 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.3287 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.3575 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.4247 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.1891 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.3734 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.2761 | test accuracy: 0.88\n",
            "Epoch:  1 | train loss: 0.2013 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.4452 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.6478 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2076 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2721 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.4451 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2912 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.3401 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2182 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.5957 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.4104 | test accuracy: 0.88\n",
            "Epoch:  1 | train loss: 0.3836 | test accuracy: 0.88\n",
            "Epoch:  1 | train loss: 0.3623 | test accuracy: 0.88\n",
            "Epoch:  1 | train loss: 0.1258 | test accuracy: 0.88\n",
            "Epoch:  1 | train loss: 0.0852 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.5967 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2788 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.2748 | test accuracy: 0.90\n",
            "Epoch:  1 | train loss: 0.3686 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.0974 | test accuracy: 0.89\n",
            "Epoch:  1 | train loss: 0.3760 | test accuracy: 0.90\n",
            "Epoch:  1 | train loss: 0.2263 | test accuracy: 0.90\n",
            "Epoch:  1 | train loss: 0.1485 | test accuracy: 0.88\n",
            "Epoch:  1 | train loss: 0.3788 | test accuracy: 0.88\n",
            "Epoch:  2 | train loss: 0.6283 | test accuracy: 0.88\n",
            "Epoch:  2 | train loss: 0.2125 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.2951 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3521 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.7403 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.1622 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.1230 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.2313 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.2747 | test accuracy: 0.88\n",
            "Epoch:  2 | train loss: 0.5918 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.0720 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.4418 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.2592 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.2468 | test accuracy: 0.87\n",
            "Epoch:  2 | train loss: 0.4635 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.2595 | test accuracy: 0.88\n",
            "Epoch:  2 | train loss: 0.3244 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.4992 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.5146 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.3199 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.1612 | test accuracy: 0.90\n",
            "Epoch:  2 | train loss: 0.3786 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.4049 | test accuracy: 0.89\n",
            "Epoch:  2 | train loss: 0.4861 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.3252 | test accuracy: 0.88\n",
            "Epoch:  3 | train loss: 0.5915 | test accuracy: 0.89\n",
            "Epoch:  3 | train loss: 0.3513 | test accuracy: 0.89\n",
            "Epoch:  3 | train loss: 0.1701 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.1632 | test accuracy: 0.89\n",
            "Epoch:  3 | train loss: 0.3207 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.3224 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2817 | test accuracy: 0.88\n",
            "Epoch:  3 | train loss: 0.2515 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2007 | test accuracy: 0.89\n",
            "Epoch:  3 | train loss: 0.2652 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2072 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2821 | test accuracy: 0.89\n",
            "Epoch:  3 | train loss: 0.2753 | test accuracy: 0.91\n",
            "Epoch:  3 | train loss: 0.3820 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.3830 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2920 | test accuracy: 0.89\n",
            "Epoch:  3 | train loss: 0.2440 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.4627 | test accuracy: 0.89\n",
            "Epoch:  3 | train loss: 0.2589 | test accuracy: 0.88\n",
            "Epoch:  3 | train loss: 0.3165 | test accuracy: 0.89\n",
            "Epoch:  3 | train loss: 0.3583 | test accuracy: 0.88\n",
            "Epoch:  3 | train loss: 0.5363 | test accuracy: 0.90\n",
            "Epoch:  3 | train loss: 0.2477 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.1817 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.1049 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.1114 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.5395 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.1294 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.4064 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.0981 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.1209 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.0911 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.3404 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2986 | test accuracy: 0.88\n",
            "Epoch:  4 | train loss: 0.4406 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.1805 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.3069 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2164 | test accuracy: 0.88\n",
            "Epoch:  4 | train loss: 0.1520 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2478 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2948 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.4360 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.4425 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.1599 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.0980 | test accuracy: 0.90\n",
            "Epoch:  4 | train loss: 0.2925 | test accuracy: 0.89\n",
            "Epoch:  4 | train loss: 0.1885 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2136 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2115 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.3013 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2108 | test accuracy: 0.89\n",
            "Epoch:  5 | train loss: 0.2972 | test accuracy: 0.89\n",
            "Epoch:  5 | train loss: 0.1565 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2859 | test accuracy: 0.89\n",
            "Epoch:  5 | train loss: 0.2136 | test accuracy: 0.89\n",
            "Epoch:  5 | train loss: 0.1233 | test accuracy: 0.89\n",
            "Epoch:  5 | train loss: 0.3667 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.8170 | test accuracy: 0.89\n",
            "Epoch:  5 | train loss: 0.4596 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.3176 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2348 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2745 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.5788 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.1936 | test accuracy: 0.89\n",
            "Epoch:  5 | train loss: 0.3767 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.4018 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2946 | test accuracy: 0.88\n",
            "Epoch:  5 | train loss: 0.4720 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.2394 | test accuracy: 0.89\n",
            "Epoch:  5 | train loss: 0.1108 | test accuracy: 0.90\n",
            "Epoch:  5 | train loss: 0.0890 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.1078 | test accuracy: 0.89\n",
            "Epoch:  6 | train loss: 0.2153 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.1597 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.3268 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.2087 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.2757 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.1907 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.3723 | test accuracy: 0.89\n",
            "Epoch:  6 | train loss: 0.1846 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.1860 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.3608 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.3083 | test accuracy: 0.89\n",
            "Epoch:  6 | train loss: 0.2998 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.1154 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.2877 | test accuracy: 0.89\n",
            "Epoch:  6 | train loss: 0.4853 | test accuracy: 0.89\n",
            "Epoch:  6 | train loss: 0.2527 | test accuracy: 0.91\n",
            "Epoch:  6 | train loss: 0.2837 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.1880 | test accuracy: 0.89\n",
            "Epoch:  6 | train loss: 0.3772 | test accuracy: 0.88\n",
            "Epoch:  6 | train loss: 0.4417 | test accuracy: 0.89\n",
            "Epoch:  6 | train loss: 0.1979 | test accuracy: 0.89\n",
            "Epoch:  6 | train loss: 0.2644 | test accuracy: 0.90\n",
            "Epoch:  6 | train loss: 0.3694 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.4947 | test accuracy: 0.89\n",
            "Epoch:  7 | train loss: 0.2130 | test accuracy: 0.89\n",
            "Epoch:  7 | train loss: 0.4387 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.4666 | test accuracy: 0.91\n",
            "Epoch:  7 | train loss: 0.1123 | test accuracy: 0.89\n",
            "Epoch:  7 | train loss: 0.2543 | test accuracy: 0.89\n",
            "Epoch:  7 | train loss: 0.1710 | test accuracy: 0.89\n",
            "Epoch:  7 | train loss: 0.1920 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.3675 | test accuracy: 0.89\n",
            "Epoch:  7 | train loss: 0.4704 | test accuracy: 0.89\n",
            "Epoch:  7 | train loss: 0.2922 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.2208 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.2969 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.3382 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.2110 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.2802 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.3986 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.1542 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.0699 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.6500 | test accuracy: 0.89\n",
            "Epoch:  7 | train loss: 0.3195 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.1287 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.4636 | test accuracy: 0.90\n",
            "Epoch:  7 | train loss: 0.3086 | test accuracy: 0.89\n",
            "Epoch:  8 | train loss: 0.3534 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.3112 | test accuracy: 0.89\n",
            "Epoch:  8 | train loss: 0.1259 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1954 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.3016 | test accuracy: 0.88\n",
            "Epoch:  8 | train loss: 0.0962 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1344 | test accuracy: 0.89\n",
            "Epoch:  8 | train loss: 0.1406 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.2367 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.2867 | test accuracy: 0.88\n",
            "Epoch:  8 | train loss: 0.2064 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.8075 | test accuracy: 0.91\n",
            "Epoch:  8 | train loss: 0.3019 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.2879 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1484 | test accuracy: 0.89\n",
            "Epoch:  8 | train loss: 0.2036 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1924 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1946 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.3516 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.3713 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.1613 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.6128 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.8817 | test accuracy: 0.90\n",
            "Epoch:  8 | train loss: 0.5959 | test accuracy: 0.91\n",
            "Epoch:  9 | train loss: 0.3104 | test accuracy: 0.89\n",
            "Epoch:  9 | train loss: 0.3527 | test accuracy: 0.89\n",
            "Epoch:  9 | train loss: 0.2582 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.1113 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.5513 | test accuracy: 0.89\n",
            "Epoch:  9 | train loss: 0.1733 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.1259 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.1626 | test accuracy: 0.89\n",
            "Epoch:  9 | train loss: 0.1456 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.2975 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.3750 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.2447 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.1428 | test accuracy: 0.89\n",
            "Epoch:  9 | train loss: 0.3327 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.2763 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.4624 | test accuracy: 0.89\n",
            "Epoch:  9 | train loss: 0.2898 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.3131 | test accuracy: 0.89\n",
            "Epoch:  9 | train loss: 0.2012 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.5016 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.1705 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.3501 | test accuracy: 0.89\n",
            "Epoch:  9 | train loss: 0.3951 | test accuracy: 0.90\n",
            "Epoch:  9 | train loss: 0.3820 | test accuracy: 0.90\n",
            "[7 2 1 0 4 1 4 9 6 9] prediction number\n",
            "[7 2 1 0 4 1 4 9 5 9] real number\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0PdWs6IsdDb"
      },
      "source": [
        "# Convolutional Neural Network (CNN) Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFxYC_DfsYZI",
        "outputId": "b99d87c0-970a-4fc2-a3af-c0a773ccad8a"
      },
      "source": [
        "class CNN(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(CNN, self).__init__()\r\n",
        "    # input shape (1, 28, 28)\r\n",
        "    self.conv1 = nn.Sequential(\r\n",
        "        nn.Conv2d(\r\n",
        "            in_channels=1,              # input height\r\n",
        "            out_channels=16,            # n_filters\r\n",
        "            kernel_size=5,              # filter size\r\n",
        "            stride=1,                   # filter movement/step\r\n",
        "            padding=2                   # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\r\n",
        "            ),                          # output shape (16, 28, 28)\r\n",
        "        nn.ReLU(),                      # activation\r\n",
        "        nn.MaxPool2d(kernel_size=2)     # choose max value in 2x2 area, output shape (16, 14, 14)\r\n",
        "        ) \r\n",
        "    # input shape (16, 14, 14)\r\n",
        "    self.conv2 = nn.Sequential(\r\n",
        "        nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 14, 14)\r\n",
        "        nn.ReLU(),                      # activation\r\n",
        "        nn.MaxPool2d(2)                 # output shape (32, 7, 7)\r\n",
        "        ) \r\n",
        "    self.out = nn.Linear(32 * 7 * 7, 10)# fully connected layer, output 10 classes\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.conv2(x)\r\n",
        "    x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\r\n",
        "    output = self.out(x)\r\n",
        "    return output, x                    # return x for visualization\r\n",
        "\r\n",
        "cnn = CNN()\r\n",
        "print(cnn)  # net architecture\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\r\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\r\n",
        "\r\n",
        "# training and testing\r\n",
        "for epoch in range(EPOCH):\r\n",
        "    for step, (b_x, b_y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\r\n",
        "\r\n",
        "        output = cnn(b_x)[0]            # cnn output\r\n",
        "        loss = loss_func(output, b_y)   # cross entropy loss\r\n",
        "        optimizer.zero_grad()           # clear gradients for this training step\r\n",
        "        loss.backward()                 # backpropagation, compute gradients\r\n",
        "        optimizer.step()                # apply gradients\r\n",
        "\r\n",
        "        if step % 50 == 0:\r\n",
        "            test_output, last_layer = cnn(test_x)\r\n",
        "            pred_y = torch.max(test_output, 1)[1].data.numpy()\r\n",
        "            accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))\r\n",
        "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\r\n",
        "           \r\n",
        "# print 10 predictions from test data\r\n",
        "test_output, _ = cnn(test_x[:10])\r\n",
        "pred_y = torch.max(test_output, 1)[1].data.numpy()\r\n",
        "print(pred_y, 'prediction number')\r\n",
        "print(test_y[:10].numpy(), 'real number')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
            ")\n",
            "Epoch:  0 | train loss: 2.3145 | test accuracy: 0.12\n",
            "Epoch:  0 | train loss: 0.6450 | test accuracy: 0.82\n",
            "Epoch:  0 | train loss: 0.2305 | test accuracy: 0.88\n",
            "Epoch:  0 | train loss: 0.1928 | test accuracy: 0.90\n",
            "Epoch:  0 | train loss: 0.1733 | test accuracy: 0.92\n",
            "Epoch:  0 | train loss: 0.1425 | test accuracy: 0.94\n",
            "Epoch:  0 | train loss: 0.1165 | test accuracy: 0.94\n",
            "Epoch:  0 | train loss: 0.0836 | test accuracy: 0.95\n",
            "Epoch:  0 | train loss: 0.1746 | test accuracy: 0.96\n",
            "Epoch:  0 | train loss: 0.0340 | test accuracy: 0.95\n",
            "Epoch:  0 | train loss: 0.2516 | test accuracy: 0.96\n",
            "Epoch:  0 | train loss: 0.1837 | test accuracy: 0.96\n",
            "Epoch:  0 | train loss: 0.0506 | test accuracy: 0.97\n",
            "Epoch:  0 | train loss: 0.0595 | test accuracy: 0.96\n",
            "Epoch:  0 | train loss: 0.1458 | test accuracy: 0.97\n",
            "Epoch:  0 | train loss: 0.1050 | test accuracy: 0.97\n",
            "Epoch:  0 | train loss: 0.0828 | test accuracy: 0.96\n",
            "Epoch:  0 | train loss: 0.0396 | test accuracy: 0.97\n",
            "Epoch:  0 | train loss: 0.0093 | test accuracy: 0.97\n",
            "Epoch:  0 | train loss: 0.0336 | test accuracy: 0.97\n",
            "Epoch:  0 | train loss: 0.0232 | test accuracy: 0.97\n",
            "Epoch:  0 | train loss: 0.0214 | test accuracy: 0.97\n",
            "Epoch:  0 | train loss: 0.0304 | test accuracy: 0.97\n",
            "Epoch:  0 | train loss: 0.2346 | test accuracy: 0.97\n",
            "Epoch:  1 | train loss: 0.1447 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0076 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0188 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0877 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0327 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0788 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0276 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.1147 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0643 | test accuracy: 0.97\n",
            "Epoch:  1 | train loss: 0.1335 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0488 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.1740 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0091 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.1040 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.1276 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.2266 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0072 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0742 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0106 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0139 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.1201 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0437 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0987 | test accuracy: 0.98\n",
            "Epoch:  1 | train loss: 0.0524 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0646 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0239 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0105 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0411 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0510 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.1846 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0135 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0140 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0343 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0029 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0148 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0242 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0260 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0374 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0034 | test accuracy: 0.99\n",
            "Epoch:  2 | train loss: 0.0604 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0284 | test accuracy: 0.99\n",
            "Epoch:  2 | train loss: 0.0947 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0482 | test accuracy: 0.99\n",
            "Epoch:  2 | train loss: 0.0118 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0098 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0039 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0744 | test accuracy: 0.98\n",
            "Epoch:  2 | train loss: 0.0479 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0285 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0150 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0457 | test accuracy: 0.99\n",
            "Epoch:  3 | train loss: 0.1032 | test accuracy: 0.99\n",
            "Epoch:  3 | train loss: 0.0033 | test accuracy: 0.99\n",
            "Epoch:  3 | train loss: 0.0088 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0156 | test accuracy: 0.99\n",
            "Epoch:  3 | train loss: 0.0027 | test accuracy: 0.99\n",
            "Epoch:  3 | train loss: 0.0031 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0393 | test accuracy: 0.99\n",
            "Epoch:  3 | train loss: 0.0582 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0160 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0127 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0020 | test accuracy: 0.99\n",
            "Epoch:  3 | train loss: 0.0130 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0474 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0089 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0049 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0029 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0060 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0423 | test accuracy: 0.98\n",
            "Epoch:  3 | train loss: 0.0346 | test accuracy: 0.99\n",
            "Epoch:  3 | train loss: 0.0141 | test accuracy: 0.99\n",
            "Epoch:  3 | train loss: 0.0033 | test accuracy: 0.98\n",
            "Epoch:  4 | train loss: 0.0023 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0095 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0067 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0056 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0923 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0060 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0153 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0032 | test accuracy: 0.98\n",
            "Epoch:  4 | train loss: 0.0686 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0683 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.1491 | test accuracy: 0.98\n",
            "Epoch:  4 | train loss: 0.0316 | test accuracy: 0.98\n",
            "Epoch:  4 | train loss: 0.0032 | test accuracy: 0.98\n",
            "Epoch:  4 | train loss: 0.0699 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0004 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0030 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.1154 | test accuracy: 0.98\n",
            "Epoch:  4 | train loss: 0.0109 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0036 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0063 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0974 | test accuracy: 0.99\n",
            "Epoch:  4 | train loss: 0.0014 | test accuracy: 0.98\n",
            "Epoch:  4 | train loss: 0.0218 | test accuracy: 0.98\n",
            "Epoch:  4 | train loss: 0.0008 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0089 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0024 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0298 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0015 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.1039 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0608 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0482 | test accuracy: 0.98\n",
            "Epoch:  5 | train loss: 0.0392 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0008 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0065 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0010 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0056 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0430 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0224 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0007 | test accuracy: 0.98\n",
            "Epoch:  5 | train loss: 0.0617 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0372 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0049 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0527 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0041 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0014 | test accuracy: 0.99\n",
            "Epoch:  5 | train loss: 0.0111 | test accuracy: 0.98\n",
            "Epoch:  5 | train loss: 0.0030 | test accuracy: 0.98\n",
            "Epoch:  5 | train loss: 0.0052 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0118 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0046 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0217 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0013 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0354 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0003 | test accuracy: 0.98\n",
            "Epoch:  6 | train loss: 0.1718 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0026 | test accuracy: 0.98\n",
            "Epoch:  6 | train loss: 0.0084 | test accuracy: 0.98\n",
            "Epoch:  6 | train loss: 0.0010 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0009 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.1079 | test accuracy: 0.98\n",
            "Epoch:  6 | train loss: 0.0026 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0023 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0009 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0043 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0180 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0110 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0013 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0099 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0002 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0026 | test accuracy: 0.99\n",
            "Epoch:  6 | train loss: 0.0017 | test accuracy: 0.98\n",
            "Epoch:  6 | train loss: 0.0149 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.1740 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0010 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0029 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0001 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0138 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0060 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0012 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0124 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0355 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0012 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0003 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0031 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0161 | test accuracy: 0.98\n",
            "Epoch:  7 | train loss: 0.0055 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0008 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0062 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0325 | test accuracy: 0.98\n",
            "Epoch:  7 | train loss: 0.0021 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0076 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0072 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0340 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0380 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0080 | test accuracy: 0.99\n",
            "Epoch:  7 | train loss: 0.0216 | test accuracy: 0.98\n",
            "Epoch:  8 | train loss: 0.0084 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0112 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0872 | test accuracy: 0.98\n",
            "Epoch:  8 | train loss: 0.0009 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0014 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0067 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0018 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0003 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0345 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0003 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0037 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0005 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0030 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0068 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0038 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.1745 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0058 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0022 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0006 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0069 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0088 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0030 | test accuracy: 0.99\n",
            "Epoch:  8 | train loss: 0.0002 | test accuracy: 0.98\n",
            "Epoch:  8 | train loss: 0.0062 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0241 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0005 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0006 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0023 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0004 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0020 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0062 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0329 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0005 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0011 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0004 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0001 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0019 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0025 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0001 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0024 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0247 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0127 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0047 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0042 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0010 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0410 | test accuracy: 0.99\n",
            "Epoch:  9 | train loss: 0.0148 | test accuracy: 0.98\n",
            "Epoch:  9 | train loss: 0.0018 | test accuracy: 0.99\n",
            "[7 2 1 0 4 1 4 9 5 9] prediction number\n",
            "[7 2 1 0 4 1 4 9 5 9] real number\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrM37KVWteGi"
      },
      "source": [
        "# Recurrent Neural Network (RNN) Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfwLfouqtiOH",
        "outputId": "7014763b-5113-43da-cdee-0ca89bc52b11"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torchvision.datasets as dsets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "# Hyper Parameters\r\n",
        "EPOCH = 1               # train the training data n times, to save time, we just train 1 epoch\r\n",
        "BATCH_SIZE = 64\r\n",
        "TIME_STEP = 28          # rnn time step / image height\r\n",
        "INPUT_SIZE = 28         # rnn input size / image width\r\n",
        "LR = 0.01               # learning rate\r\n",
        "DOWNLOAD_MNIST = True   # set to True if haven't download the data\r\n",
        "\r\n",
        "# Mnist digital dataset\r\n",
        "train_data = dsets.MNIST(\r\n",
        "    root='./mnist/',\r\n",
        "    train=True,                         # this is training data\r\n",
        "    transform=transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\r\n",
        "    download=DOWNLOAD_MNIST,            # download it if you don't have it\r\n",
        ")\r\n",
        "\r\n",
        "# plot one example\r\n",
        "print(train_data.train_data.size())     # (60000, 28, 28)\r\n",
        "print(train_data.train_labels.size())   # (60000)\r\n",
        "\r\n",
        "# Data Loader for easy mini-batch return in training\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\r\n",
        "\r\n",
        "# convert test data into Variable, pick 2000 samples to speed up testing\r\n",
        "test_data = dsets.MNIST(root='./mnist/', train=False, transform=transforms.ToTensor())\r\n",
        "test_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255.   # shape (2000, 28, 28) value in range(0,1)\r\n",
        "test_y = test_data.test_labels.numpy()[:2000]    # covert to numpy array\r\n",
        "\r\n",
        "class RNN(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(RNN, self).__init__()\r\n",
        "\r\n",
        "    # if use nn.RNN(), it hardly learns\r\n",
        "    self.rnn = nn.LSTM(\r\n",
        "        input_size=INPUT_SIZE,\r\n",
        "        hidden_size=64,         # rnn hidden unit\r\n",
        "        num_layers=1,           # number of rnn layer\r\n",
        "        batch_first=True\r\n",
        "        )                       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\r\n",
        "\r\n",
        "    self.out = nn.Linear(64, 10)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state      \r\n",
        "    out = self.out(r_out[:, -1, :])         # choose r_out at the last time step\r\n",
        "    return out\r\n",
        "\r\n",
        "rnn = RNN()\r\n",
        "print(rnn)\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\r\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\r\n",
        "\r\n",
        "# training and testing\r\n",
        "for epoch in range(EPOCH):\r\n",
        "    for step, (b_x, b_y) in enumerate(train_loader):    # gives batch data\r\n",
        "        b_x = b_x.view(-1, 28, 28)                      # reshape x to (batch, time_step, input_size)\r\n",
        "\r\n",
        "        output = rnn(b_x)                               # rnn output\r\n",
        "        loss = loss_func(output, b_y)                   # cross entropy loss\r\n",
        "        optimizer.zero_grad()                           # clear gradients for this training step\r\n",
        "        loss.backward()                                 # backpropagation, compute gradients\r\n",
        "        optimizer.step()                                # apply gradients\r\n",
        "\r\n",
        "        if step % 50 == 0:\r\n",
        "            test_output = rnn(test_x)                   # (samples, time_step, input_size)\r\n",
        "            pred_y = torch.max(test_output, 1)[1].data.numpy()\r\n",
        "            accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)\r\n",
        "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\r\n",
        "\r\n",
        "# print 10 predictions from test data\r\n",
        "test_output = rnn(test_x[:10].view(-1, 28, 28))\r\n",
        "pred_y = torch.max(test_output, 1)[1].data.numpy()\r\n",
        "print(pred_y, 'prediction number')\r\n",
        "print(test_y[:10], 'real number')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n",
            "RNN(\n",
            "  (rnn): LSTM(28, 64, batch_first=True)\n",
            "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:63: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 | train loss: 2.3185 | test accuracy: 0.10\n",
            "Epoch:  0 | train loss: 1.2478 | test accuracy: 0.54\n",
            "Epoch:  0 | train loss: 0.8551 | test accuracy: 0.69\n",
            "Epoch:  0 | train loss: 0.6400 | test accuracy: 0.73\n",
            "Epoch:  0 | train loss: 0.4108 | test accuracy: 0.81\n",
            "Epoch:  0 | train loss: 0.3168 | test accuracy: 0.87\n",
            "Epoch:  0 | train loss: 0.3157 | test accuracy: 0.86\n",
            "Epoch:  0 | train loss: 0.1488 | test accuracy: 0.89\n",
            "Epoch:  0 | train loss: 0.1660 | test accuracy: 0.92\n",
            "Epoch:  0 | train loss: 0.1526 | test accuracy: 0.93\n",
            "Epoch:  0 | train loss: 0.1760 | test accuracy: 0.94\n",
            "Epoch:  0 | train loss: 0.2752 | test accuracy: 0.94\n",
            "Epoch:  0 | train loss: 0.1345 | test accuracy: 0.94\n",
            "Epoch:  0 | train loss: 0.3160 | test accuracy: 0.92\n",
            "Epoch:  0 | train loss: 0.2366 | test accuracy: 0.92\n",
            "Epoch:  0 | train loss: 0.0706 | test accuracy: 0.95\n",
            "Epoch:  0 | train loss: 0.2403 | test accuracy: 0.96\n",
            "Epoch:  0 | train loss: 0.1124 | test accuracy: 0.94\n",
            "Epoch:  0 | train loss: 0.2435 | test accuracy: 0.94\n",
            "[7 2 1 0 4 1 4 9 8 9] prediction number\n",
            "[7 2 1 0 4 1 4 9 5 9] real number\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}